<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Juuso's blog on Open Data Science</title>
 <link href="http://ouzor.github.io/blog/atom.xml" rel="self"/>
 <link href="http://ouzor.github.io/"/>
 <updated>2019-03-30T10:21:51+00:00</updated>
 <id>http://ouzor.github.io/blog</id>
 <author>
   <name>Juuso Parkkinen</name>
   <email>juuso.parkkinen@iki.fi</email>
 </author>
 
 
 <entry>
   <title>Learning To Run</title>
   <link href="http://ouzor.github.io/blog/2019/03/27/Learning-To-Run.html"/>
   <updated>2019-03-27T08:00:00+00:00</updated>
   <id>id:/blog/2019/03/27/Learning To Run"</id>
   <content type="html">&lt;p&gt;In this article we present our approach for the NIPS 2017 ”Learning To Run” challenge. The goal of the challenge is to develop a controller able to run in a complex environment, by training a model with Deep Reinforcement Learning methods. We follow the approach of the team Reason8 (3rd place). We begin from the algorithm that performed better on the task, Deep Deterministic Policy Gradient (DDPG). We implement and benchmark several improvements over vanilla DDPG, including parallel sampling, parameter noise, layer normalization and domain specific changes. We were able to reproduce results of the Reason8 team, obtaining a model able to run for more than 30m.&lt;/p&gt;

&lt;center&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/HVOrhxypOGg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Reinforcement Learning (RL) deals with sequential descision making problems. At each time step the agent observes the world state, selects an action and receives a reward.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/high_level.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 1 - RL&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The figure above represents the Reinforcement Learning framework. The agent receives, at each time step a representation \( s \) of the state of the system. The agent takes an action a, according to its policy \( \pi \). The action has an effect on the world. The effect is a transition to the state \( s’ \), according to the world dynamic \( P \). At the same time the agent receives a reward \( r \), based on the action and on the state.
The policy \( \pi \) is often encoded in a neural network.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/high_level2.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The RL goal is to maximize the expected discounted sum of rewards:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J_\pi = \mathbb{E}\left[\sum_{t=0}^H \gamma^t r(s_t,a_t) \right] \, .&lt;/script&gt;

&lt;p&gt;Where \( J \) represents the objective function, \( \gamma \) is the discount factor, \( r \) is the reward and \( H \) is the horizon length.&lt;/p&gt;

&lt;h3 id=&quot;how-can-we-achieve-this-goal&quot;&gt;How can we achieve this goal?&lt;/h3&gt;
&lt;p&gt;Gradient ascent over policy parameters:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta' = \theta + \eta \nabla_\theta J_\pi .&lt;/script&gt;

&lt;p&gt;Where the update of the policy parameters \( \theta \)  follows the gradient of the objective function \( \nabla J). 
A straightforward approach to accomplish this is presented in &lt;a class=&quot;citation&quot; href=&quot;#Williams1992&quot;&gt;(Williams, 1992)&lt;/a&gt; with REINFORCE algorithm, that uses Monte Carlo sampling to estimate the performance gradient considering a stochastic policy. Deterministic Policy Gradient (DPG) &lt;a class=&quot;citation&quot; href=&quot;#dpg&quot;&gt;(Silver et al., 2014)&lt;/a&gt; expands on this by considering deterministic policies only, for continuous action spaces. To ensure adequate exploration, an off-policy actor-critic algorithm is introduced to learn a deterministic target policy from an exploratory behavior policy. However, directly using neural networks as function approximators leads to unsatisfactory results due to two problems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;most optimization algorithms for neural networks assume that samples are independently and identically distributed, which is not true when samples are generated from exploring sequentially in an environment;&lt;/li&gt;
  &lt;li&gt;since the network Q, part of Q-learning algorithms, being updated is also used in calculating the target value, the Q update is prone to divergence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DQN [3] implements Q-learning using a deep neural network to approximate the Q function. To address the problem (1.) DQN introduces a replay buffer which stores transitions generated by the environment. During training, a batch of transitions is drawn from the buffer to restore the i.i.d. property. Although, since a maximization over the action space is required, DQN does not scale with high-dimensional and continuous action spaces.
Deep Deterministic Policy Gradient (DDPG) [4] solves all the these problems by using:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A deterministic parametrization of the actor \( \pi \), updated in the direction of the gradient of \( Q(s, \pi(s)) \). This is a scalable alternative to global maximization, that is infeasible in  high-dimensional continuous action spaces;&lt;/li&gt;
  &lt;li&gt;A replay buffer;&lt;/li&gt;
  &lt;li&gt;Separated target networks with soft-updates to improve convergence stability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deep-deterministic-policy-gradient&quot;&gt;Deep Deterministic Policy Gradient&lt;/h2&gt;

&lt;p&gt;DDPG is a state of the art algorithm in Deep Reinforcement Learning.&lt;/p&gt;
&lt;h3 id=&quot;off-policy&quot;&gt;Off policy&lt;/h3&gt;
&lt;p&gt;DDPG is an off-policy method, which means that the optimized policy is different from the behavioral policy. Off-policy algorithm usually allow re-usage of all the samples, whereas on-policy approaches would require to throw them away at each update.&lt;/p&gt;
&lt;h3 id=&quot;actor-critic&quot;&gt;Actor Critic&lt;/h3&gt;
&lt;p&gt;Actor critic algorithm uses two networks. The actor network represents the agent policy and outputs an action given a state. The critic network takes as input the pair state action and outputs an estimates of the quality of the action in that state. The two networks used in our project are the following:&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/actor_critic.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Critic is trained using off-policy data coming from the replay buffer, that is a FIFO queue containing tuples(st, at, rt, st+1). Critic’s task is to minimize the Bellman error (notice that the policy is deterministic, so we can avoid the expectation over actions):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(s_t,a_t \mid \theta^Q ) = r(s_t,a_t) + \gamma Q(s_{t+1}, \pi (s_{t+1}  \mid \theta^\pi ) \mid \theta^Q )&lt;/script&gt;

&lt;p&gt;where \( \theta^Q \) are parameters of the critic network and \( \theta^\pi \) are actor’s parameters.
It is evident in the equation above that the update step of the weights \( \theta^Q \) comprises \( Q(s_t, a_t \mid \theta^Q) \) in the target, resulting in an iterative update prone to divergence. DDPG solves this problem using target networks. Target networks are copies of the actor and critic networks that are only used for computing target values, and softly updated for improving stability:
&lt;script type=&quot;math/tex&quot;&gt;\theta' = \tau \theta + (1-\tau)\theta' \qquad \tau \ll 1,&lt;/script&gt;
where \( \theta \) are the weights of the original network and \( \theta’ \) the weights of the target networks.&lt;/p&gt;

&lt;p&gt;The resulting error for the critic network is:
&lt;script type=&quot;math/tex&quot;&gt;L = \frac{1}{N} \sum_{i=1}^N (y_i - Q(s_i, a_i \mid \theta^Q))^2 \, ,&lt;/script&gt;
with target
&lt;script type=&quot;math/tex&quot;&gt;y_i = r_i + \gamma Q(s_i, \pi(s_i \mid \theta'^\pi) \mid \theta'^{Q}).&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The actor is updated using the estimated deterministic policy gradient, here reported for the sake of completeness:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\theta^\pi}J \approx \frac{1}{N} \sum_{i=1}^N \nabla_a Q(s, a \mid \theta^Q) \mid_{s = s_i, a = \pi(s_i)} \nabla_{\theta^\pi} \pi(s \mid \theta^\pi)\mid_{s_i}.&lt;/script&gt;

&lt;p&gt;Applying the vanilla DDPG algorithm to the learning to run task leads to unsatisfactory results as we can see from the initial video.&lt;/p&gt;

&lt;h2 id=&quot;ddpg-improvements&quot;&gt;DDPG Improvements&lt;/h2&gt;

&lt;h3 id=&quot;parallel-sampling&quot;&gt;Parallel Sampling&lt;/h3&gt;
&lt;p&gt;A key step in any reinforcement learning algorithm is the generation of \( (s_t, a_t, r_t, s_{t+1}) \) transitions to gather information from the environment. Since our simulator is very slow, making this step as fast as possible is desirable. Parallelization in our algorithm is implemented through three types of threads: sampling threads, training thread and evaluation threads.
Each sampling thread is tasked with collecting trajectories using the provided policy, pushing them in a common queue and waiting for a new policy.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/ddpg.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/ddpg_focus_1.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The training thread waits samples from \( m \) sampling threads, stores them in the replay buffer and trains the actor and critic networks for a fixed number of training steps. The new actor network is then sent to the waiting sampling threads that can now restart the sampling process.
Evaluation threads are spawned every fixed number of training steps.
Having multiple sampling threads running policies with different weights improves exploration and results in a substantial performance improvement. We used 20 sampling threads, 1 training thread and 5 evaluation threads in our implementation. We found out in our experiments that \( m=1 \) is a good trade-off between sampling and training.&lt;/p&gt;

&lt;h3 id=&quot;exploration&quot;&gt;Exploration&lt;/h3&gt;
&lt;p&gt;To explore we used action noise and parameter noise \cite{param_noise} in an alternated way. At the beginning of an episode we selected between action noise and parameter noise with 0.7 and 0.3 probability respectively.
Action noise is directly applied to the action selected by the actor network.
We used an Ornstein-Uhlenbeck (OU) \cite{ou_noise} process to generate correlated noise for efficient exploration in physics based environments.
Parameter noise perturbs actor network weights to obtain a state dependent exploration, thus more coherent with respect to action noise. The noise used in parameter noise is sampled at the beginning of an episode and it’s kept fixed for all the episode. Parameter noise works well with layer normalization \cite{layer_normalization}. 
Layer normalization, as the name says, normalizes the output of a selected layer.
This technique, besides stabilizing training, makes possible to use the same perturbation scale across all network layers. We used layer normalization both for actor and critic networks applying it to all layer except the last one before the non linearity.&lt;/p&gt;

&lt;h3 id=&quot;states-and-actions-flip&quot;&gt;States and actions flip&lt;/h3&gt;
&lt;p&gt;The model has a symmetric body, thus it’s easy to increase the sample size by flipping states and actions. Flipping a state means to swap left and right parts of the body, flipping an action means to swap actuations of left and right legs.
States and actions flip is implemented in this way: we sample transitions  \( (s_t, a_t, r_t, s_{t+1}) \) from the replay buffer, flip state components of \( s_t \) and \( s_{t+1} \), flip the action  \( a_t \) and add to the batch original as well as flipped transition. This has an easy interpretation: we know that if action \( a_t \)  in state \( s_t \) , results in state  \( s_{t+1} \)  and in a reward signal \( r_t \), doing the symmetric action with respect to \( a_t\) in the symmetric state with respect to \( s_t \) results in the symmetric state with respect to \( s_{t+1} \) and in the &lt;em&gt;same&lt;/em&gt; reward signal \( r_t \). 
Flipping transitions helps in obtaining symmetric policies, that is desirable since running is a symmetric task.&lt;/p&gt;

&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;/h2&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/our_problem.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The agent is a musculoskeletal model including information about muscles, joints and links moving in a 2D environment (no movement is possible along Z axis).
Kinematic quantities are provided for body parts and joints, while activation, fiber length and fiber velocity are provided for muscles.&lt;br /&gt;
The total amount of variables describing the state of the agent is 146.
The agent can actuate 9 muscles for each leg, thus \( a \in [0,1]^{18} \). 
In our version of the environment we removed obstacles, obtaining a slightly easier version of the task. 
Reward is defined as the change in the x coordinate of the pelvis for each step plus a small penalty for using ligament forces.
An episode terminates when the agent falls (pelvis \( y &amp;lt; 0.65 \)) or after 1000 steps.
Simulation is based on the OpenSim library that relies on the SimBody physics engine. Due to a complex physics engine the environment is quite slow compared to standard RL environments (Roboschool, Mojoco, etc.) and a step can take seconds, thus it is crucial to use the most sample-efficient method.&lt;/p&gt;

&lt;h3 id=&quot;environment-modifications&quot;&gt;Environment modifications&lt;/h3&gt;
&lt;p&gt;We used several modifications of the environment during training to improve efficiency and to help the learning algorithm.&lt;/p&gt;

&lt;h4 id=&quot;reward&quot;&gt;Reward&lt;/h4&gt;
&lt;p&gt;We added a small bonus to the reward for each time-step survived. We did not study the contribution of this change thoroughly, but we expect it to add some greediness to the initial training steps to favor policies that keeps the model standing.&lt;/p&gt;

&lt;h4 id=&quot;environment-accuracy&quot;&gt;Environment accuracy&lt;/h4&gt;
&lt;p&gt;We used a lower integrator accuracy with respect to the standard one of the simulator obtaining 5x speedup. After some episodes the environment becomes slower with respect to initial episodes, probably for a memory leak. We solved this problem doing a reset of the environment after 100 episodes.&lt;/p&gt;

&lt;h4 id=&quot;relative-positions&quot;&gt;Relative positions&lt;/h4&gt;
&lt;p&gt;As mentioned above, position vectors of the model body parts are exposed by the simulator as absolute. For the purpose of learning, keeping an absolute reference frame is undesirable. In fact, being running an approximately periodic task, having the skeleton in some position at a given distance from the origin, should make no difference from having it in the same position at another distance. Therefore, we modified the observation vector by centering the \( x \) coordinates of the body parts around the &lt;strong&gt;pelvis&lt;/strong&gt; \( x \). Exploiting such symmetry of the model enables shorter learning time and, most importantly, higher generalization.&lt;/p&gt;

&lt;h4 id=&quot;state-variables&quot;&gt;State variables&lt;/h4&gt;
&lt;p&gt;OpenSim exposes a number of variables for a musculoskeletal model. Even though to preserve the Markovian property we should consider them all, many of them are in practice useless for the task to learn. In training our model, two subsets of them were considered and we refer to them as &lt;strong&gt;full-state&lt;/strong&gt; and &lt;strong&gt;reduced-state&lt;/strong&gt;.
&lt;strong&gt;Reduced-state&lt;/strong&gt; comprises the \( x \), \( y\) position of body parts, the rotation and rotational speed of joints, the speed and position of the center of mass, resulting in \( s \in \mathbb{R}^{34}\). Namely body parts are &lt;strong&gt;head, torso, right and left toes and talus&lt;/strong&gt; and joints are &lt;strong&gt;ground pelvis and left and right ankles, hips and knees&lt;/strong&gt;. 
&lt;strong&gt;Full-state&lt;/strong&gt; takes into account all the variables from &lt;strong&gt;reduced-state&lt;/strong&gt;, together with the speed and acceleration of body parts and acceleration of joints, resulting in \( s \in \mathbb{R}^{67} \).&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;For all the experiments we ran DDPG algorithm with the modifications we describe in sections \ref{sec:proposed} and \ref{sec:environment}. We performed an &lt;strong&gt;ablation study&lt;/strong&gt; to test the relevance of our main changes to vanilla implementation. We compared the performance of a model trained on the &lt;strong&gt;reduced-state&lt;/strong&gt; space with respect to the &lt;strong&gt;full-state&lt;/strong&gt; space. Moreover we compared the quality of the models learned with or without state-action-flip and parameter noise, in terms of performance and required training steps.
All the models running on the &lt;strong&gt;reduced-state&lt;/strong&gt; configuration share the same architecture for actor and critic networks, with Xavier initialization \cite{pmlr-v9-glorot10a} for the neurons.&lt;/p&gt;

&lt;h3 id=&quot;state-action-flip-and-parameter-noise&quot;&gt;State action flip and parameter noise&lt;/h3&gt;
&lt;p&gt;In this experiment we investigated on the importance of &lt;strong&gt;state-action flip&lt;/strong&gt; and &lt;strong&gt;parameter noise&lt;/strong&gt; (PN) for the learning process. We trained four models for approximately $10^6$ training steps with all the combinations of the two improvements, i.e. with and without state-action flip and parameter noise. Performance statistics are reported in figure \ref{fig:flip-pn}. From our experimental results, introducing both modifications leads to both better performance, in terms of longer run distance, and a significant speed-up in terms of training steps to reach same distance. 
It is also worth highlighting that the learned model with state-action flip achieved higher performance than the one with PN only. This possibly remarks the importance of domain-specific additions in the context of RL which outperformed an uninformed exploration.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/learning_curves.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;sampling-threads&quot;&gt;Sampling threads&lt;/h3&gt;
&lt;p&gt;In this experiment we analyzed the impact of the number of sampling threads. We trained two models with 10 and 20 sampling threads respectively. We used the same sampling-training strategy: wait for samples from 1 thread, check the state of the other threads (collecting samples if available), train for 300 steps, send the updated policies to waiting threads that restart the sampling process. Figure \ref{fig:thread-number}
shows that, as expected, the experiment with 20 sampling threads outperformed the experiment with 10  sampling threads. This shows the importance of exploration in this task, as well as the importance of parallelization.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/thread_number.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;h3 id=&quot;full-vs-reduces-state&quot;&gt;Full vs Reduces state&lt;/h3&gt;
&lt;p&gt;In this experiment we compared the performance of two learned models, the first trained over &lt;strong&gt;full-state&lt;/strong&gt; space and the second over &lt;strong&gt;reduced-state&lt;/strong&gt; space. The former was trained using a \( [150, 64] \) &lt;strong&gt;elu&lt;/strong&gt; actor network and a \( [150, 50]\) &lt;strong&gt;tanh&lt;/strong&gt; critic network. The latter was trained with a \( [64, 64] \) &lt;strong&gt;elu&lt;/strong&gt; actor network and a \( [64, 32] \) &lt;strong&gt;tanh&lt;/strong&gt; critic network. Performance statistics are reported in figure \ref{fig:full-reduced}. From our experiments, models trained with a \textit{reduce-state} space outperformed those trained with the bigger state space. \textit{Full-state} space introduces several variables that could help in learning a controller for our task, but they also increase the complexity of the model. We did not test thoroughly the networks architecture for the \textit{full-state} space and incrementing the number of neurons might lead to better performance.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/blog/figs/l2run/reduced_vs_full.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - RL&quot; /&gt;
&lt;/center&gt;

&lt;h2 id=&quot;authors&quot;&gt;Authors&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Emanuele Ghelfi&lt;/li&gt;
  &lt;li&gt;Leonardo Arcari&lt;/li&gt;
  &lt;li&gt;Emiliano Gagliardi&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Williams1992&quot;&gt;Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. &lt;i&gt;Machine Learning&lt;/i&gt;, &lt;i&gt;8&lt;/i&gt;(3), 229–256. https://doi.org/10.1007/BF00992696&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dpg&quot;&gt;Silver et al., D. (2014). Deterministic Policy Gradient Algorithms. &lt;i&gt;ICML&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>Every Experiment is Sacred</title>
   <link href="http://ouzor.github.io/blog/2019/02/12/Every-Experiment-is-sacred.html"/>
   <updated>2019-02-12T08:00:00+00:00</updated>
   <id>id:/blog/2019/02/12/Every Experiment is sacred</id>
   <content type="html">&lt;p&gt;Managing Machine Learning experiments is usually painful.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/sacred/sacred3.jpg&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 1 - Experiments&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The usual workflow when approaching a problem using Machine Learning tools is the following. You study the problem, define a possible solution, then you implement that solution and measure its quality. Often the solution depends on some parameters, and we refer to the set of parameters with the term &lt;strong&gt;configuration&lt;/strong&gt;. Parameters can include model type, optimizers, learning rate, batch size, steps, losses, and many others.&lt;/p&gt;

&lt;p&gt;The configuration profoundly influences the performance of your solution. If you have enough time and computational power, it is possible to use a Bayesian approach for solving the hyper-parameter selection problem, but in real situations, it is common to perform a limited set of experiment and select the best configuration among them.&lt;/p&gt;

&lt;p&gt;In this article we describe how to manage experiments in a good way, ensuring inspectability and reproducibility. We focus on Machine Learning experiments in python using &lt;a href=&quot;www.tensorflow.com&quot;&gt;Tensorflow&lt;/a&gt; even if this approach applies to all computational experiments.&lt;/p&gt;

&lt;h2 id=&quot;simple-bad-solution&quot;&gt;Simple (Bad) Solution&lt;/h2&gt;

&lt;p&gt;To keep track of the configuration the usual way to go (at least in Tensorflow) is to save your models and logs inside a folder with a shared pattern for the parameters name and value.&lt;/p&gt;

&lt;p&gt;Using this setting with &lt;a href=&quot;https://www.tensorflow.org/guide/summaries_and_tensorboard&quot;&gt;Tensorboard&lt;/a&gt; (the official Tensorflow visualization tool), it is possible to relate the plot to its configuration. However, this is &lt;strong&gt;not&lt;/strong&gt; the right way to manage experiments since it is tough to see how parameters affect performance. Tensorboard offers none way to order experiments by configuration, selecting only some experiments or order experiments by performance.
In this way, we do not have any control and tracking of our source code, except for our VCS. Unfortunately, when doing experiments, we do not always push our changes since they might be tiny changes or only trials, this can cause issues since our experiments might not be reproducible.
We can implement an experiment using this style in the following way:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# other imports ...
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_logdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;_{key}_{config[key]}&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# load the configuration
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# load dataset, model, summaries, loss using the configuration
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# merge summaries
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;summary_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# define a logdir to keep track of the configuration
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logs/{get_logdir(config)}&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# instantiate a tensorflow saver
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Saver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# train loop
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'steps'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# your training code ...
&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# write summaries
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# save model
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code snippet, we load the model, dataset and other possible configurations, the logdir defined is needed for associating a configuration to the right plot in Tensorboard.
In this way we obtain the following folder structure using only 4 parameters:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── lr=0.0002_batch_size=32_kernel_size=5_steps=100
├── lr=0.0001_batch_size=32_kernel_size=5_steps=100
├── lr=0.0002_batch_size=32_kernel_size=7_steps=100
├── lr=0.0002_batch_size=16_kernel_size=7_steps=100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Logging experiments in this way is not very comfortable, and it is not the right way to manage experiments and make them reproducible.&lt;/p&gt;

&lt;h2 id=&quot;sacred-solution&quot;&gt;Sacred Solution&lt;/h2&gt;

&lt;blockquote class=&quot;blockquote&quot;&gt;&lt;p&gt;
Every experiment is sacred &lt;br /&gt;
Every experiment is great &lt;br /&gt;
If an experiment is wasted &lt;br /&gt;
God gets quite irate
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://sacred.readthedocs.io/en/latest/index.html&quot;&gt;Sacred&lt;/a&gt; is a tool that lets you configure, organize, and log computational experiments. It is designed to introduce a minimal overhead and code addition.&lt;/p&gt;

&lt;p&gt;With sacred you can:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Keep track of all parameters of your experiment&lt;/li&gt;
  &lt;li&gt;Run your experiment with different settings&lt;/li&gt;
  &lt;li&gt;Save configuration and results in files or database&lt;/li&gt;
  &lt;li&gt;Reproduce your results.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sacred dumps and saves &lt;strong&gt;everything&lt;/strong&gt; into MongoDB including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Metrics (training/validation loss, accuracy, or anything else you decide to log)&lt;/li&gt;
  &lt;li&gt;Console output&lt;/li&gt;
  &lt;li&gt;All the source code you executed&lt;/li&gt;
  &lt;li&gt;All the imported library with their versions used at runtime&lt;/li&gt;
  &lt;li&gt;All your configuration parameters&lt;/li&gt;
  &lt;li&gt;Hardware spec of your host&lt;/li&gt;
  &lt;li&gt;Random seeds&lt;/li&gt;
  &lt;li&gt;Artifacts and resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To visualize and interact with your experiments a nice visualization board for this tool is &lt;a href=&quot;https://github.com/vivekratnavel/omniboard&quot;&gt;Omniboard&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/blog/figs/sacred/omniboard4.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 1 - Omniboard&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;em&gt;Figure 1 - Omniboard&lt;/em&gt;
&lt;!-- ![Figure 1 - Omniboard](/blog/figs//omniboard4.png &quot;Figure 1 - Omniboard&quot;){:class=&quot;blog-image&quot;}
&lt;/div&gt; --&gt;&lt;/p&gt;

&lt;!-- &lt;div markdown=&quot;1&quot; class=&quot;blog-image-container&quot;&gt;
![Figure 2 - Omniboard - Experiment details](/blog/figs//omniboard5.png &quot;Figure 2 - Omniboard - Experiment details&quot;){:class=&quot;blog-image&quot;}
&lt;/div&gt; --&gt;
&lt;center&gt;
&lt;img src=&quot;/blog/figs/sacred/omniboard5.png&quot; style=&quot;width: 80%;&quot; alt=&quot;Figure 2 - Omniboard - Experiment Details&quot; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;em&gt;Figure 2 - Omniboard - Experiment details&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Omniboard lets you tag, annotate and order experiments making inspection and model selection easy.&lt;/p&gt;

&lt;h2 id=&quot;sacred-integration&quot;&gt;Sacred Integration&lt;/h2&gt;

&lt;p&gt;Integrating sacred in your code is painless. The previous code becomes:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# other imports ...
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# tensorflow
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.python.training.summary_io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SummaryWriterCache&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# sacred
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sacred&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Experiment&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sacred.observers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MongoObserver&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# instantiate a sacred experiment
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Experiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;experiment_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# add the MongoDB observer
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MongoObserver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# load the configuration
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;default.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;experiment.json&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;automain&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogFileWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# load dataset, model, summaries, loss using the parameters
&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# merge summaries
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;summary_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# define a logdir using the experiment id of sacred
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;logs/{_run._id}&quot;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# instantiate a tensorflow saver
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Saver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SummaryWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# train loop
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# your training code ...
&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# log loss to sacred
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;training.loss&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;summaries&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# write summaries as tensorflow logs
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# save model
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this way, we create a new instance of &lt;code class=&quot;highlighter-rouge&quot;&gt;Experiment&lt;/code&gt; and an instance of &lt;code class=&quot;highlighter-rouge&quot;&gt;MongoObserver&lt;/code&gt; to store data to MongoDB. We add two different configurations, a default configuration (&lt;code class=&quot;highlighter-rouge&quot;&gt;default.json&lt;/code&gt;) and an experiment configuration (&lt;code class=&quot;highlighter-rouge&quot;&gt;experiment.json&lt;/code&gt;). The function decorated with &lt;code class=&quot;highlighter-rouge&quot;&gt;@ex.automain&lt;/code&gt; is the main function of the experiment and Sacred automatically injects its parameters. The &lt;code class=&quot;highlighter-rouge&quot;&gt;LogFileWriter(ex)&lt;/code&gt; decorator is used to store the location of summaries produced by Tensorflow (created by &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow.summary.FileWriter&lt;/code&gt;) into the experiment record specified by the ex argument. Whenever a new &lt;code class=&quot;highlighter-rouge&quot;&gt;FileWriter&lt;/code&gt; instantiation is detected in a scope of the decorator or the context manager, the path of the log is copied to the experiment record exactly as passed to the &lt;code class=&quot;highlighter-rouge&quot;&gt;FileWriter&lt;/code&gt;. The location(s) can be then found under &lt;code class=&quot;highlighter-rouge&quot;&gt;info[&quot;tensorflow&quot;][&quot;logdirs&quot;]&lt;/code&gt; of the experiment.&lt;/p&gt;

&lt;p&gt;Using this approach the folder structure is clean (folders are named with a progressive id) and we detach the folder name from the actual experiment configuration.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Managing experiments in a more efficient way is a crucial priority, especially when doing research. Reproducibility is an essential requirement for computational studies including
those based on machine learning techniques. Sacred is a potent tool, straightforward to integrate into your codebase, saving you the effort of developing a custom way to keep track of your experiments.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org&quot;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/guide/summaries_and_tensorboard&quot;&gt;Tensorboard&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sacred.readthedocs.io/en/latest/index.html&quot;&gt;Sacred&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IDSIA/sacred&quot;&gt;Sacred repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/vivekratnavel/omniboard&quot;&gt;Omniboard&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@u39kun/managing-your-machine-learning-experiments-and-making-them-repeatable-in-tensorflow-pytorch-bc8043099dbd&quot;&gt;Medium Article&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;disclosure&quot;&gt;Disclosure&lt;/h2&gt;
&lt;p&gt;This article has been posted on the &lt;a href=&quot;https://blog.zuru.tech/machine-learning/2019/02/12/every-experiment-is-sacred&quot;&gt;Zuru Tech Italy&lt;/a&gt; blog first and cross-posted here.&lt;/p&gt;
</content>
 </entry>
 
 
</feed>